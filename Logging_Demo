Logging Demo
============
Key Success Factor: Had to delete my incorrect credentials set up from the first course
AND complete the kops walkthrough of a sample cluster to ensure my local kops, and
remote aws account was set up correctly:
 >   https://github.com/kubernetes/kops/blob/master/docs/aws.md

First clone the repo:

ubuntu@ubuntu-xenial:~$ git clone https://github.com/wardviaene/advanced-kubernetes-course.git

Will use the files in the logging subdirectory

ubuntu@ubuntu-xenial:~$ git clone https://github.com/wardviaene/advanced-kubernetes-course.git

ls -la
total 52
drwxrwxr-x  2 ubuntu ubuntu  4096 Jan  6 00:42 .
drwxrwxr-x 13 ubuntu ubuntu  4096 Jan  6 00:42 ..
-rw-rw-r--  1 ubuntu ubuntu   382 Jan  6 00:42 es-service.yaml
-rw-rw-r--  1 ubuntu ubuntu  3085 Jan  6 00:42 es-statefulset.yaml
-rw-rw-r--  1 ubuntu ubuntu 13462 Jan  6 00:42 fluentd-es-configmap.yaml
-rw-rw-r--  1 ubuntu ubuntu  2846 Jan  6 00:42 fluentd-es-ds.yaml
-rw-rw-r--  1 ubuntu ubuntu  1336 Jan  6 00:42 kibana-deployment.yaml
-rw-rw-r--  1 ubuntu ubuntu   375 Jan  6 00:42 kibana-service.yaml
-rw-rw-r--  1 ubuntu ubuntu   392 Jan  6 00:42 README.md
-rw-rw-r--  1 ubuntu ubuntu   159 Jan  6 00:42 storage.yml


Will have to launch a new AWS Cluster with kops.

First remove your old AWS cluster with kops if you haven't already done so...
REMOVE CLUSTER COMMAND
----------------------
  kops delete cluster --name kubernetes.name.info  --state=s3://name --yes


The parameters in the kops command are as follows:

The old command in the first course:
------------------------------------

kops create cluster --name=xxx--state=s3://kops-state-xxx --zones=us-east-1a --node-count=2 --node-count=2 --node-size=t2.micro --master-size=t2.micro --dns-zone=


Command required attributes.
AWS_PROFILE=kops  (I didn't need the AWS_PROFILE name)
kops create cluster
--name=
--state=s3:
--zones=
--node-count=3 --node-size=t2.medium --master-size=t2.micro
--dns-zone=

FIRST ATTEMPT:

ERROR:
------
error reading cluster configuration "

EC2RoleRequestError: no EC2 instance role found
caused by: RequestError: send request failed
caused by: Get http://169.254.169.254/latest/meta-data/iam/security-credentials: dial tcp 169.254.169.254:80: getsockopt: network is unreachable

Fixed by deleted the incorrect cluster:
***************************************

New command:
Set variables for ease of use.

kops create cluster --name=$NAME  --state=$KOPS_STATE_STORE   --zones=us-east-1a --node-count=3 --node-size=t2.medium --master-size=t2.micro --dns-zone=$DNS_ZONE

Stdout:
Must specify --yes to apply changes

Cluster configuration has been created.

Suggestions:
 * list clusters with: kops get cluster
 * edit this cluster with: kops edit cluster kubernetes.jbrent.info
 * edit your node instance group: kops edit ig --name=kubernetes.jbrent.info nodes
 * edit your master instance group: kops edit ig --name=kubernetes.jbrent.info master-us-east-1a

Finally configure your cluster with: kops update cluster kubernetes.jbrent.info --yes


IF YOU AGREE WITH THE CONFIGURATION THEN TYPE:
                        --- THIS next COMMAND will launch the cluster:

kops update cluster  --name=$NAME     --yes

NOTE:   volume type gp2     = EBS General Purpose SSD (gp2)*


Confirm that it is done correctly
---------------------------------

- look AWS Console
- kops validate cluster

kops validate cluster   (must wait an appropriate time for the cluster to spin up)
Using cluster from kubectl context: kubernetes.jbrent.info

Validating cluster kubernetes.jbrent.info

Validation Failed

The dns-controller Kubernetes deployment has not updated the Kubernetes cluster's API DNS entry
to the correct IP address.  The API DNS IP address is the placeholder address that kops creates: 203.0.113.123.
Please wait about 5-10 minutes for a master to start, dns-controller to launch, and DNS to propagate.
The protokube container and dns-controller deployment logs may contain more diagnostic information.
Etcd and the API DNS entries must be updated for a kops Kubernetes cluster to start.

**** ANSWER - I had to delete the cluster, and the security groups and re-launch it.
I noticed in the AWS Console that the nodes security group was never created.
I also had to manually delete the key for the kubernetes cluster BEFORE
creating anew cluster.

kops delete cluster --name=$NAME

kubectl get nodes --show-lables

kubectl get nodes --show-labels
NAME                            STATUS    ROLES     AGE       VERSION
etc
I was unable to connect to it because of DNS issues - again...

================================================================================

I then blew it away and attempted to re-create it: this was the response around 1600 hrs.

kops create cluster --name=$NAME  --state=$KOPS_STATE_STORE   --zones=us-east-1a --node-count=3 --node-size=t2.medium --master-size=t2.micro --dns-zone=$DNS_ZONE --yes

cluster "kubernetes.jbrent.info" already exists; use 'kops update cluster' to apply changes
ubuntu@ubuntu-xenial:~/.aws$ kops delete cluster --name=$NAME --yes

ubuntu@ubuntu-xenial:~/.aws$ kops delete cluster --name=$NAME --yes

No cloud resources to delete

Deleted cluster: "kubernetes.jbrent.info"

================================================================================
I then  re-built it at 1920 hours.

This is what happened when I attempted to validate.
kops validate cluster
Using cluster from kubectl context: kubernetes.jbrent.info

Validating cluster kubernetes.jbrent.info
0930 hours
Validation Failed

The dns-controller Kubernetes deployment has not updated the Kubernetes cluster's API DNS entry to the correct IP address.
The API DNS IP address is the placeholder address that kops creates: 203.0.113.123.
Please wait about 5-10 minutes for a master to start, dns-controller to launch, and DNS to propagate.
The protokube container and dns-controller deployment logs may contain more diagnostic information.
Etcd and the API DNS entries must be updated for a kops Kubernetes cluster to start.

Cannot reach cluster's API server: unable to Validate Cluster: kubernetes.jbrent.info

So I waited until the following morning and then it seemed to work.

***************** So it took about 3 hours for the cluster DNS to propogate !!!!

kubectl get nodes
NAME                            STATUS    ROLES     AGE       VERSION
ip-172-20-36-178.ec2.internal   Ready     node      10h       v1.8.6
ip-172-20-40-253.ec2.internal   Ready     node      10h       v1.8.6
ip-172-20-42-22.ec2.internal    Ready     master    11h       v1.8.6
ip-172-20-51-73.ec2.internal    Ready     node      10h       v1.8.6

kops validate cluster
Using cluster from kubectl context: kubernetes.jbrent.info

Validating cluster kubernetes.jbrent.info

INSTANCE GROUPS
NAME			ROLE	MACHINETYPE	MIN	MAX	SUBNETS
master-us-east-1a	Master	t2.micro	1	1	us-east-1a
nodes			Node	t2.medium	3	3	us-east-1a

NODE STATUS
NAME				ROLE	READY
ip-172-20-36-178.ec2.internal	node	True
ip-172-20-40-253.ec2.internal	node	True
ip-172-20-42-22.ec2.internal	master	True
ip-172-20-51-73.ec2.internal	node	True

Your cluster kubernetes.jbrent.info is ready

Success....


 kubectl get nodes
NAME                            STATUS    ROLES     AGE       VERSION
ip-172-20-36-178.ec2.internal   Ready     node      11h       v1.8.6
ip-172-20-40-253.ec2.internal   Ready     node      11h       v1.8.6
ip-172-20-42-22.ec2.internal    Ready     master    11h       v1.8.6
ip-172-20-51-73.ec2.internal    Ready     node      11h       v1.8.6

================================================================================

MUST Label nodes first....
== label each node using kubectl with the phrase   fluentd-ds-ready=true


manually label the nodes:
kubectl label nodes <node-name> <label-key>=<label-value>

ip-172-20-36-178.ec2.internal   Ready     node      12h       v1.8.6
ip-172-20-40-253.ec2.internal   Ready     node      12h       v1.8.6
ip-172-20-42-22.ec2.internal    Ready     master    12h       v1.8.6
ip-172-20-51-73.ec2.internal    Ready     node      12h       v1.8.6

kubectl label nodes   ip-172-20-36-178.ec2.internal  fluentd-ds-ready=true
kubectl label nodes   ip-172-20-40-253.ec2.internal fluentd-ds-ready=true
kubectl label nodes   ip-172-20-42-22.ec2.internal  fluentd-ds-ready=true
kubectl label nodes   ip-172-20-51-73.ec2.internal  fluentd-ds-ready=true

kubectl get nodes --show-labels
NAME                            STATUS    ROLES     AGE       VERSION   LABELS
ip-172-20-36-178.ec2.internal   Ready     node      12h       v1.8.6    beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=t2.medium,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=us-east-1,failure-domain.beta.kubernetes.io/zone=us-east-1a,fluentd-ds-ready=true,kops.k8s.io/instancegroup=nodes,kubernetes.io/hostname=ip-172-20-36-178.ec2.internal,kubernetes.io/role=node,node-role.kubernetes.io/node=
ip-172-20-40-253.ec2.internal   Ready     node      12h       v1.8.6    beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=t2.medium,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=us-east-1,failure-domain.beta.kubernetes.io/zone=us-east-1a,fluentd-ds-ready=true,kops.k8s.io/instancegroup=nodes,kubernetes.io/hostname=ip-172-20-40-253.ec2.internal,kubernetes.io/role=node,node-role.kubernetes.io/node=
ip-172-20-42-22.ec2.internal    Ready     master    13h       v1.8.6    beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=t2.micro,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=us-east-1,failure-domain.beta.kubernetes.io/zone=us-east-1a,fluentd-ds-ready=true,kops.k8s.io/instancegroup=master-us-east-1a,kubernetes.io/hostname=ip-172-20-42-22.ec2.internal,kubernetes.io/role=master,node-role.kubernetes.io/master=
ip-172-20-51-73.ec2.internal    Ready     node      12h       v1.8.6    beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=t2.medium,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=us-east-1,failure-domain.beta.kubernetes.io/zone=us-east-1a,fluentd-ds-ready=true,kops.k8s.io/instancegroup=nodes,kubernetes.io/hostname=ip-172-20-51-73.ec2.internal,kubernetes.io/role=node,node-role.kubernetes.io/node=

correct label: beta.kubernetes.io/fluentd-ds-ready=true


kubectl label node <nodename> <labelname>-

kubectl label nodes   ip-172-20-36-178.ec2.internal  beta.kubernetes.io/fluentd-ds-ready=true
kubectl label nodes   ip-172-20-40-253.ec2.internal beta.kubernetes.io/fluentd-ds-ready=true
kubectl label nodes   ip-172-20-42-22.ec2.internal  beta.kubernetes.io/fluentd-ds-ready=true
kubectl label nodes   ip-172-20-51-73.ec2.internal  beta.kubernetes.io/fluentd-ds-ready=true


Modifications to the set up of the files for the AWS region etc.
----------------------------------------------------------------

Modify storage.yaml for the parameter: zone
in my cast is us east 1a    aka: --zones=us-east-1a

Now we can deploy.

kubectl -f create .    # creates all in directory.

kubectl create -f .
service "elasticsearch-logging" created
serviceaccount "elasticsearch-logging" created
clusterrole "elasticsearch-logging" created
clusterrolebinding "elasticsearch-logging" created
statefulset "elasticsearch-logging" created
configmap "fluentd-es-config-v0.1.0" created
serviceaccount "fluentd-es" created
clusterrole "fluentd-es" created
clusterrolebinding "fluentd-es" created
daemonset "fluentd-es-v2.0.1" created
deployment "kibana-logging" created
service "kibana-logging" created
storageclass "standard" created

Being created ---------


Check the status of the cluster creation
-----------------------------------------
>>>  this time wait for a while before trying...

kubectl get pods
No resources found.

MUST ADD THE NAMESPACE TO THE get pods query
> because unless the namespace is defined in .kube/config...
 >>> kubectl defaults to the "default" namespace.

 kubectl get pods --namespace=kube-system
 NAME                                                   READY     STATUS    RESTARTS   AGE
 dns-controller-f75fbb996-ccg5q                         1/1       Running   0          11h
 elasticsearch-logging-0                                1/1       Running   0          23m
 elasticsearch-logging-1                                1/1       Running   0          22m
 etcd-server-events-ip-172-20-42-22.ec2.internal        1/1       Running   0          11h
 etcd-server-ip-172-20-42-22.ec2.internal               1/1       Running   0          11h
 kibana-logging-78867c787f-56l46                        1/1       Running   0          23m
 kube-apiserver-ip-172-20-42-22.ec2.internal            1/1       Running   46         11h
 kube-controller-manager-ip-172-20-42-22.ec2.internal   1/1       Running   0          11h
 kube-dns-7f56f9f8c7-4mzlq                              3/3       Running   0          11h
 kube-dns-7f56f9f8c7-r8ngd                              3/3       Running   0          11h
 kube-dns-autoscaler-f4c47db64-2dcwb                    1/1       Running   0          11h
 kube-proxy-ip-172-20-36-178.ec2.internal               1/1       Running   0          11h
 kube-proxy-ip-172-20-40-253.ec2.internal               1/1       Running   0          11h
 kube-proxy-ip-172-20-42-22.ec2.internal                1/1       Running   0          11h
 kube-proxy-ip-172-20-51-73.ec2.internal                1/1       Running   0          11h
 kube-scheduler-ip-172-20-42-22.ec2.internal            1/1       Running   0          11h

etc...

NOTICE THE 46 RESTARTS of the apiserver.

Had to delete the deployment because I forgot to label the nodes...
then labeled the nodes and re-deployed...
STRANGE the age of the pods did not change....


kubectl get pods --namespace=kube-system
NAME                                                   READY     STATUS    RESTARTS   AGE
dns-controller-f75fbb996-ccg5q                         1/1       Running   0          13h
elasticsearch-logging-0                                1/1       Running   0          6m
elasticsearch-logging-1                                1/1       Running   0          5m
etcd-server-events-ip-172-20-42-22.ec2.internal        1/1       Running   0          13h
etcd-server-ip-172-20-42-22.ec2.internal               1/1       Running   0          13h
kibana-logging-78867c787f-k2jfw                        1/1       Running   0          6m
kube-apiserver-ip-172-20-42-22.ec2.internal            1/1       Running   46         13h
kube-controller-manager-ip-172-20-42-22.ec2.internal   1/1       Running   0          13h
kube-dns-7f56f9f8c7-4mzlq                              3/3       Running   0          13h
kube-dns-7f56f9f8c7-r8ngd                              3/3       Running   0          13h
kube-dns-autoscaler-f4c47db64-2dcwb                    1/1       Running   0          13h
kube-proxy-ip-172-20-36-178.ec2.internal               1/1       Running   0          13h
kube-proxy-ip-172-20-40-253.ec2.internal               1/1       Running   0          13h
kube-proxy-ip-172-20-42-22.ec2.internal                1/1       Running   0          13h
kube-proxy-ip-172-20-51-73.ec2.internal                1/1       Running   0          13h
kube-scheduler-ip-172-20-42-22.ec2.internal            1/1       Running   0          13h

Successful .........................

notice two pods for elasticsearch... statefulset adds the ordinal to the name
and there is no random number generate behind the normal pod name
elasticsearch-logging-0
elasticsearch-logging-1



kubectl get pv --namespace=kube-system


The claims volumes are attached to elasticsearch correctly:


kubectl get pv --namespace=kube-system
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                                            STORAGECLASS   REASON    AGE
pvc-c788722a-xxx   8Gi        RWO            Delete           Bound     kube-system/es-storage-elasticsearch-logging-0   standard                 1h
pvc-f3f7220a-xxx   8Gi        RWO            Delete           Bound     kube-system/es-storage-elasticsearch-logging-1   standard                 1h



Now go to AWS to look at the load balancer...

The nodes are in EC2

The load balancer is there...

The hostname is the DNS name (A record)
- you can create a Route53 record
kibana.yourcluster.com to point to this load balancer DNS name.

You must edit the security group (scroll to the bottom of the lower pane in the page)
 and you only allow your own IP address

 sg-xxxxxxxxxxxxxxxxxx

copy the DNS record and add the port: 5601 for kibana

a1509d8e609c411e8884712963a833af-1776922874.us-east-1.elb.amazonaws.com:5601

http://ad1xxxxxxX

and you will see the kibana interface.

In my case I was unable to fetch a Time Filter field name so I could not create the first index.
in Kibana
I had to chose no timestamp pattern and found a field called:
timeFieldName Then I had to blow away the index and use createDate.

I got the following error when in kibana
Time Filter field name: createDate
This page lists every field in the * index and the field's associated core type as
recorded by Elasticsearch. While this list allows you to view the core type of each field,
changing field types must be done using Elasticsearch's Mapping API

No matching indices found: No indices match pattern "kibana"â€‹

Error: No matching indices found: No indices match pattern "kibana"
KbnError@http://ad1e016

In this particular case I had to manually label the nodes and didn't label them with the prefix in error.
 - once I added the correct label I delete the deployment files
 - and then redeployed the files the Fluentd pods were correctly created and this all worked.



.....



kops delete cluster --name kubernetes.newtech.academy --state=s3://kops-state-cccccccc
kops create cluster --name=$NAME  --state=$KOPS_STATE_STORE   --zones=us-east-1a --node-count=3 --node-size=t2.medium --master-size=t2.micro --dns-zone=$DNS_ZONE

kops delete cluster --name=$NAME  --state=$KOPS_STATE_STORE
(remember this must be done in the same terminal session you installed or you have to reset the vars)



VolumeType          	gp2  =  EBS General Purpose SSD (gp2)*







....
